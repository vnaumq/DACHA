{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Импорт библиотек для работы с файловой системой и изображениями\n",
    "import os                       # Работа с файловой системой\n",
    "import cv2                      # Библиотека OpenCV для обработки изображений\n",
    "import matplotlib.pyplot as plt # Визуализация данных и графиков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 1: Импорт библиотек для работы с файловой системой и изображениями\n",
    "- `os`: предоставляет функции для взаимодействия с операционной системой (пути, файлы)\n",
    "- `cv2`: OpenCV, используется для обработки изображений (чтение, изменение размера и т.д.)\n",
    "- `matplotlib.pyplot` (plt): инструмент для построения графиков и отображения изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 2: Импорт библиотек для машинного обучения и массивов\n",
    "import numpy as np              # Работа с массивами и математическими операциями\n",
    "import tensorflow as tf         # Основная библиотека машинного обучения\n",
    "from tensorflow import keras    # Высокоуровневый API для нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 2: Импорт библиотек для машинного обучения и массивов\n",
    "- `numpy` (np): эффективная работа с многомерными массивами и числовыми вычислениями\n",
    "- `tensorflow` (tf): фреймворк для построения и обучения моделей машинного обучения\n",
    "- `keras`: модуль TensorFlow для упрощенного создания и управления нейронными сетями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 3: Импорт специфичных компонентов Keras\n",
    "from tensorflow.keras.datasets import fashion_mnist     # Набор данных Fashion MNIST\n",
    "from tensorflow.keras.models import Sequential, load_model  # Модель и загрузка моделей\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D  # Слои нейронной сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 3: Импорт специфичных компонентов Keras\n",
    "- `fashion_mnist`: встроенный набор данных с изображениями одежды (60,000 + 10,000 примеров)\n",
    "- Импорт из `tensorflow.keras.models`:\n",
    "  - `Sequential`: класс для последовательного добавления слоев в модель\n",
    "  - `load_model`: функция для загрузки сохраненных моделей\n",
    "- Импорт из `tensorflow.keras.layers`:\n",
    "  - `Dense`: полносвязный слой\n",
    "  - `Flatten`: преобразование многомерных данных в вектор\n",
    "  - `Dropout`: слой выброса для регуляризации\n",
    "  - `Conv2D`: сверточный слой для обработки изображений\n",
    "  - `MaxPooling2D`: слой субдискретизации для уменьшения размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 4: Определение пути к модели\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))              # Базовая директория скрипта\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'files', 'model', 'fashion_mnist_model.h5')  # Путь к файлу модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 4: Определение пути к модели\n",
    "- `BASE_DIR`: вычисляет директорию, в которой находится текущий скрипт\n",
    "  - `os.path.abspath(__file__)`: полный путь к файлу скрипта\n",
    "  - `os.path.dirname()`: извлекает директорию из пути\n",
    "- `MODEL_PATH`: полный путь для сохранения/загрузки модели\n",
    "  - Использует `os.path.join()` для кроссплатформенной совместимости\n",
    "  - Модель будет храниться в подкаталоге `files/model` с именем `fashion_mnist_model.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 5: Загрузка данных Fashion MNIST\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()  # Загрузка данных\n",
    "x_train = x_train.reshape((60000, 28, 28, 1))                     # Изменение формы обучающих данных\n",
    "x_test = x_test.reshape((10000, 28, 28, 1))                       # Изменение формы тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 5: Загрузка данных Fashion MNIST\n",
    "- `fashion_mnist.load_data()`: загружает набор данных Fashion MNIST\n",
    "  - Возвращает кортеж из двух пар:\n",
    "    - `(x_train, y_train)`: 60,000 обучающих изображений и меток\n",
    "    - `(x_test, y_test)`: 10,000 тестовых изображений и меток\n",
    "- `reshape()`: добавляет канал для совместимости со сверточными сетями\n",
    "  - Исходная форма: (количество_примеров, 28, 28)\n",
    "  - Новая форма: (количество_примеров, 28, 28, 1)\n",
    "    - 28x28 - размер изображения\n",
    "    - 1 - канал (градации серого)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 6: Нормализация данных\n",
    "x_train = x_train / 255.0  # Нормализация обучающих данных\n",
    "x_test = x_test / 255.0    # Нормализация тестовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 6: Нормализация данных\n",
    "- Деление значений пикселей на 255.0 (максимальное значение в градациях серого)\n",
    "- Преобразует диапазон значений из [0, 255] в [0, 1]\n",
    "- Улучшает сходимость модели и стабильность обучения нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 7: Проверка и загрузка/создание модели\n",
    "if os.path.exists(MODEL_PATH):  # Проверка наличия модели\n",
    "    print(\"Загрузка существующей модели...\")\n",
    "    model = load_model(MODEL_PATH)  # Загрузка сохраненной модели\n",
    "else:\n",
    "    print(\"Создание новой модели...\")\n",
    "    model = Sequential([  # Создание последовательной модели\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Первый сверточный слой\n",
    "        Conv2D(64, (3, 3), activation='relu'),  # Второй сверточный слой\n",
    "        MaxPooling2D((2, 2)),  # Первый слой субдискретизации\n",
    "        Dropout(0.5),  # Первый слой выброса\n",
    "        Conv2D(128, (3, 3), activation='relu'),  # Третий сверточный слой\n",
    "        MaxPooling2D((2, 2)),  # Второй слой субдискретизации\n",
    "        Dropout(0.5),  # Второй слой выброса\n",
    "        Flatten(),  # Преобразование в вектор\n",
    "        Dense(128, activation='relu'),  # Полносвязный слой\n",
    "        Dense(10, activation='softmax')  # Выходной слой\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 7: Проверка и загрузка/создание модели\n",
    "- `os.path.exists(MODEL_PATH)`: проверяет, существует ли файл модели\n",
    "- Если модель существует:\n",
    "  - `load_model(MODEL_PATH)`: загружает сохраненную модель\n",
    "- Если модели нет:\n",
    "  - `Sequential`: создание новой модели с последовательными слоями:\n",
    "    - `Conv2D(32, (3, 3))`: 32 фильтра 3x3, входной слой с формой (28, 28, 1)\n",
    "    - `Conv2D(64, (3, 3))`: 64 фильтра 3x3\n",
    "    - `MaxPooling2D((2, 2))`: уменьшение размерности (с 28x28 до 14x14)\n",
    "    - `Dropout(0.5)`: выброс 50% нейронов\n",
    "    - `Conv2D(128, (3, 3))`: 128 фильтров 3x3\n",
    "    - `MaxPooling2D((2, 2))`: уменьшение до 7x7\n",
    "    - `Dropout(0.5)`: еще один выброс 50%\n",
    "    - `Flatten()`: преобразование в вектор\n",
    "    - `Dense(128)`: полносвязный слой с 128 нейронами\n",
    "    - `Dense(10, 'softmax')`: выходной слой для 10 классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 8: Компиляция и обучение модели (при создании)\n",
    "if not os.path.exists(MODEL_PATH):  # Выполняется только если модель создается\n",
    "    model.compile(  # Компиляция модели\n",
    "        optimizer='adam',  # Оптимизатор Adam\n",
    "        loss='sparse_categorical_crossentropy',  # Функция потерь\n",
    "        metrics=['accuracy']  # Метрика оценки\n",
    "    )\n",
    "    history = model.fit(  # Обучение модели\n",
    "        x_train, y_train,\n",
    "        epochs=15,  # Количество эпох\n",
    "        batch_size=512,  # Размер пакета\n",
    "        shuffle=True,  # Перемешивание данных\n",
    "        validation_split=0.1  # Доля валидационных данных\n",
    "    )\n",
    "    model.save(MODEL_PATH)  # Сохранение модели\n",
    "    print(\"Модель сохранена в файл:\", MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 8: Компиляция и обучение модели\n",
    "- Условно выполняется, если модель создается заново\n",
    "- `model.compile()`:\n",
    "  - `optimizer='adam'`: адаптивный оптимизатор Adam\n",
    "  - `loss='sparse_categorical_crossentropy'`: для многоклассовой классификации\n",
    "  - `metrics=['accuracy']`: отслеживание точности\n",
    "- `model.fit()`:\n",
    "  - Обучает модель на `x_train`, `y_train`\n",
    "  - `epochs=15`: 15 проходов по данным\n",
    "  - `batch_size=512`: 512 примеров в пакете\n",
    "  - `shuffle=True`: перемешивание данных\n",
    "  - `validation_split=0.1`: 10% данных для валидации\n",
    "- `model.save()`: сохраняет модель в `MODEL_PATH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 9: Оценка и визуализация результатов (при создании)\n",
    "if not os.path.exists(MODEL_PATH):  # Выполняется только если модель обучалась\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test)  # Оценка на тестовых данных\n",
    "    print('Test accuracy:', test_acc)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))  # Создание фигуры для графиков\n",
    "\n",
    "    plt.subplot(1, 2, 1)  # График потерь\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)  # График точности\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()  # Оптимизация расположения\n",
    "    plt.show()  # Отображение графиков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 9: Оценка и визуализация результатов\n",
    "- Условно выполняется, если модель обучалась\n",
    "- `model.evaluate()`: вычисляет потери и точность на тестовых данных\n",
    "- Визуализация:\n",
    "  - `plt.figure(figsize=(12, 5))`: размер фигуры 12x5 дюймов\n",
    "  - Первый график (`subplot(1, 2, 1)`): потери\n",
    "    - `history.history['loss']`: потери на обучении\n",
    "    - `history.history['val_loss']`: потери на валидации\n",
    "  - Второй график (`subplot(1, 2, 2)`): точность\n",
    "    - `history.history['accuracy']`: точность на обучении\n",
    "    - `history.history['val_accuracy']`: точность на валидации\n",
    "  - `tight_layout()`: улучшает расположение графиков\n",
    "  - `show()`: отображает графики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 10: Функция предобработки изображений\n",
    "def load_and_preprocess_images(folder_path):  # Функция загрузки изображений\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder_path):  # Перебор файлов в папке\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Чтение в градациях серого\n",
    "        img = cv2.resize(img, (28, 28))  # Изменение размера\n",
    "        img = img.reshape((28, 28, 1))  # Добавление канала\n",
    "        img = img / 255.0  # Нормализация\n",
    "        images.append(img)\n",
    "        filenames.append(filename)\n",
    "    return np.array(images), filenames  # Возврат массивов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 10: Функция предобработки изображений\n",
    "- `load_and_preprocess_images(folder_path)`: загружает и обрабатывает изображения\n",
    "- Процесс:\n",
    "  - Перебирает файлы в `folder_path` с помощью `os.listdir`\n",
    "  - `cv2.imread(..., cv2.IMREAD_GRAYSCALE)`: читает изображения в градациях серого\n",
    "  - `cv2.resize()`: приводит к размеру 28x28\n",
    "  - `reshape()`: добавляет канал (28, 28, 1)\n",
    "  - Нормализация: деление на 255.0\n",
    "- Возвращает:\n",
    "  - `np.array(images)`: массив обработанных изображений\n",
    "  - `filenames`: список имен файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 11: Загрузка и классификация изображений\n",
    "images, filenames = load_and_preprocess_images(BASE_DIR + '/files/ten_clothing_images_resized/inv')  # Загрузка изображений\n",
    "predictions = model.predict(images)  # Предсказание классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 11: Загрузка и классификация изображений\n",
    "- `load_and_preprocess_images()`: загружает изображения из указанной папки\n",
    "  - Путь: `BASE_DIR + '/files/ten_clothing_images_resized/inv'`\n",
    "- `model.predict(images)`: классифицирует загруженные изображения\n",
    "  - Возвращает массив вероятностей для каждого класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 12: Вывод результатов классификации\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']  # Названия классов\n",
    "for i, (pred, filename) in enumerate(zip(predictions, filenames)):  # Перебор предсказаний\n",
    "    print(pred)  # Вывод вероятностей\n",
    "    print(pred.max())  # Максимальная вероятность\n",
    "    predicted_class = np.argmax(pred)  # Индекс предсказанного класса\n",
    "    confidence = pred[predicted_class]  # Уверенность\n",
    "    print(f\"File: {filename}, Predicted class: {class_names[predicted_class]}, Confidence: {confidence:.2f}\")\n",
    "    print(\"=\"*100)  # Разделитель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 12: Вывод результатов классификации\n",
    "- `class_names`: список названий классов Fashion MNIST\n",
    "- Цикл по предсказаниям:\n",
    "  - `zip(predictions, filenames)`: парное объединение предсказаний и имен файлов\n",
    "  - `pred`: массив вероятностей для одного изображения\n",
    "  - `np.argmax(pred)`: индекс класса с максимальной вероятностью\n",
    "  - `confidence`: значение вероятности для предсказанного класса\n",
    "- Выводит:\n",
    "  - Все вероятности (`pred`)\n",
    "  - Максимальную вероятность (`pred.max()`)\n",
    "  - Имя файла, предсказанный класс и уверенность (с 2 знаками после запятой)\n",
    "  - Разделитель из 100 символов \"=\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
